{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe804650-b386-4238-ad2a-eb1b1471f20a",
   "metadata": {},
   "source": [
    "# 預處理思路\n",
    "從驗證的public dataset中可以看到需要從日期、站點ID、時間資料去進行每20分鐘的可用腳踏車數量的預測，因此訓練資料集必須整理成含有以上資訊的資料集，因為保持數據集可處理的彈性，以下程式碼我會分階段進行，主要是把經緯度的資料加進去並去除無效資料，但我還是有讓處理過程都輸出成csv檔案，因為驗證的公式需要tot(停車柱的數量)的資料，最後的train_data有兩種主要是保留時序的彈性。\n",
    "\n",
    "訓練資料集處理後的特徵包含date, station_id, time, latitude, longitude希望用這五個資料去預測sbi，因此驗證數據集也需要整理成這格式，看之後模型要怎麼處理這些特徵值。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e4cd65-f4cf-403b-bc7c-7f8c4250608e",
   "metadata": {},
   "source": [
    "## ENV Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb802a77-b19a-4740-843f-cf88aea6f3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa974ed-2af0-4828-8397-89a4dc17a57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbd8bcb-6db0-4629-af2a-1b5f321511a7",
   "metadata": {},
   "source": [
    "## Data Processing and Feature Acquisition\n",
    "* 輸出檔名為aggregated_data_YYYYMMDD的csv檔案到根目錄，資料很多會花一些時間\n",
    "* 輸出完成後我手動在根目錄新建一個名為\"aggreated_data\"的資料夾把這些csv檔案都放進去 (我懶得用程式)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af0b90c-e650-4286-827c-45f8d39a24d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def convert_time_to_minutes(time_str):\n",
    "    hours, minutes = map(int, time_str.split(':'))\n",
    "    return hours * 60 + minutes\n",
    "\n",
    "def process_json_file(json_path, site_id):\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Get the sorted list of times from the data\n",
    "    sorted_times = sorted(data.keys(), key=lambda x: convert_time_to_minutes(x))\n",
    "\n",
    "    # Find the first non-empty data entry\n",
    "    for time in sorted_times:\n",
    "        if data[time]:\n",
    "            last_valid_data = data[time]\n",
    "            break\n",
    "    else:\n",
    "        last_valid_data = {'tot': 0, 'sbi': 0, 'bemp': 0, 'act': '0'}\n",
    "\n",
    "    # Process data for every 20 minutes interval\n",
    "    processed_data = []\n",
    "    for minutes in range(0, 24 * 60, 20):\n",
    "        current_time_str = f\"{minutes // 60:02d}:{minutes % 60:02d}\"\n",
    "        if current_time_str in data and data[current_time_str]:\n",
    "            last_valid_data = data[current_time_str]\n",
    "\n",
    "        processed_data.append({\n",
    "            'station_id': site_id,\n",
    "            'Time (minutes)': minutes,\n",
    "            'tot': last_valid_data.get('tot', 0),\n",
    "            'sbi': last_valid_data.get('sbi', 0),\n",
    "            'bemp': last_valid_data.get('bemp', 0),\n",
    "            'act': last_valid_data.get('act', '0')\n",
    "        })\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "def main():\n",
    "    base_path = '/data/html.2023.final.data-release/release/'\n",
    "    start_date = datetime(2023, 10, 2)\n",
    "    end_date = datetime(2023, 12, 8)\n",
    "\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        folder_name = current_date.strftime('%Y%m%d')\n",
    "        folder_path = os.path.join(base_path, folder_name)\n",
    "        if os.path.exists(folder_path):\n",
    "            all_data = []\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                if file_name.endswith('.json'):\n",
    "                    json_path = os.path.join(folder_path, file_name)\n",
    "                    # Extract site ID from file name\n",
    "                    site_id = file_name.split('.')[0]\n",
    "                    all_data.extend(process_json_file(json_path, site_id))\n",
    "\n",
    "            # Convert the aggregated data into a DataFrame\n",
    "            if all_data:\n",
    "                df = pd.DataFrame(all_data)\n",
    "                output_csv = f'/data/aggregated_data_{folder_name}.csv'\n",
    "                df.to_csv(output_csv, index=False)\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43624bf-4b8c-497e-9519-b9380bcf1b32",
   "metadata": {},
   "source": [
    "## Capture latitude and longitude data from demographic\n",
    "* 提取demographic檔案經緯度資料並輸出csv檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b3b168-4f35-443e-bb96-2337669d92c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The previous path used was incorrect. Let's correct the file path.\n",
    "demographic_file_path = '/data/html.2023.final.data-release/demographic.json'\n",
    "\n",
    "# Function to extract station ID, latitude, and longitude and output it as a CSV\n",
    "def extract_station_info_to_csv(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Prepare data for DataFrame\n",
    "    stations_data = [{\n",
    "        'station_id': sno,\n",
    "        'latitude': info['lat'],\n",
    "        'longitude': info['lng']\n",
    "    } for sno, info in data.items()]\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(stations_data)\n",
    "    \n",
    "    # Define the CSV output path\n",
    "    output_csv_path = '/data/station_info.csv'\n",
    "    \n",
    "    # Save as CSV\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    \n",
    "    return output_csv_path\n",
    "\n",
    "# Call the function and get the path of the created CSV\n",
    "csv_output_path = extract_station_info_to_csv(demographic_file_path)\n",
    "csv_output_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c64d3b-a730-4ca8-832c-00b714d802ff",
   "metadata": {},
   "source": [
    "## Batch aggreated data and merge date\n",
    "* 批次處理\"aggreated_data\"資料夾內的檔案並與經緯度合併 > 最後輸出一個包含所有資料的csv資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb45a9f-1a59-46b5-b68d-3e2f9b476f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the base directory containing the aggregated data files\n",
    "base_dir = '/data/aggreated_data/'\n",
    "\n",
    "# Load station information\n",
    "station_info_file_path = '/data/station_info.csv'\n",
    "station_info_df = pd.read_csv(station_info_file_path)\n",
    "station_info_df['station_id'] = station_info_df['station_id'].astype(int)\n",
    "\n",
    "# Initialize an empty DataFrame to store the merged data\n",
    "all_data_merged = pd.DataFrame()\n",
    "\n",
    "# Iterate through each file in the directory\n",
    "for file_name in os.listdir(base_dir):\n",
    "    if file_name.startswith('aggregated_data_') and file_name.endswith('.csv'):\n",
    "        # Extract the date from the filename\n",
    "        date_str = file_name[len('aggregated_data_'):-4]\n",
    "        file_path = os.path.join(base_dir, file_name)\n",
    "\n",
    "        # Load the aggregated data file\n",
    "        aggregated_data_df = pd.read_csv(file_path)\n",
    "        aggregated_data_df['station_id'] = aggregated_data_df['station_id'].astype(int)\n",
    "\n",
    "        # Add the extracted date to the DataFrame\n",
    "        # Convert date to the desired format without separators (YYYYMMDD)\n",
    "        aggregated_data_df['date'] = pd.to_datetime(date_str).strftime('%Y%m%d')\n",
    "\n",
    "        # Merge with station information\n",
    "        merged_df = pd.merge(aggregated_data_df, station_info_df, how='left', on='station_id')\n",
    "\n",
    "        # Append to the overall DataFrame\n",
    "        all_data_merged = pd.concat([all_data_merged, merged_df])\n",
    "\n",
    "# Reset the index of the final DataFrame\n",
    "all_data_merged.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save the final merged data to a new CSV file\n",
    "output_file_path = '/data/merged_all_data.csv'\n",
    "all_data_merged.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Output file path for download\n",
    "output_file_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05553d2b-c557-409b-9065-1d7fa6bcbcec",
   "metadata": {},
   "source": [
    "## Sort columns\n",
    "* 單純看不順眼sort一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f5c2d6-d862-40ad-9be2-2c5e9b761074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import the necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the final merged dataset again due to the environment reset\n",
    "final_merged_dataset_path = '/data/merged_all_data.csv'\n",
    "final_merged_dataset = pd.read_csv(final_merged_dataset_path)\n",
    "\n",
    "# Reorder the columns according to the new requirement\n",
    "final_merged_dataset = final_merged_dataset[['date', 'station_id', 'Time (minutes)', 'latitude', 'longitude', 'tot', 'sbi', 'bemp', 'act']]\n",
    "\n",
    "# Rename the columns for consistency\n",
    "final_merged_dataset.rename(columns={'Time (minutes)': 'time'}, inplace=True)\n",
    "\n",
    "# Sort the DataFrame by 'date' and 'station_id'\n",
    "final_merged_dataset.sort_values(by=['date', 'station_id'], inplace=True)\n",
    "\n",
    "# Save the sorted and reordered DataFrame to a new CSV file\n",
    "sorted_reordered_csv_path = '/data/merged_all_data_sort.csv'\n",
    "final_merged_dataset.to_csv(sorted_reordered_csv_path, index=False)\n",
    "\n",
    "sorted_reordered_csv_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6572d4c6-1877-46b9-8312-6f3fb419798d",
   "metadata": {},
   "source": [
    "## Remove tot, bemp, act\n",
    "* 去除三項值用以符合驗證資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d46e837-12a6-4a6a-8d96-53323e5fd834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = '/data/merged_all_data_sort.csv'  # Replace this with your file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Remove specified columns\n",
    "data_cleaned = data.drop(['tot', 'bemp', 'act'], axis=1)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "data_cleaned.to_csv('train_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca6fa46-523d-451d-bb97-0a1265e30407",
   "metadata": {},
   "source": [
    "## Convert minute into 00:00 (optional)\n",
    "* 將時間轉換成24小時制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd48501-0350-410c-b85a-594b63488141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = '/data/train_data.csv'  # Replace with your file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Function to convert time format\n",
    "def convert_time(time):\n",
    "    hours = time // 60\n",
    "    minutes = time % 60\n",
    "    return f\"{hours:02d}:{minutes:02d}\"\n",
    "\n",
    "# Apply the conversion to the 'time' column\n",
    "data['time'] = data['time'].apply(convert_time)\n",
    "\n",
    "# The 'time' column is now in the desired format\n",
    "# You can now work with this updated dataframe or save it to a new file\n",
    "# For example, to save:\n",
    "data.to_csv('train_data_time_convert.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0664f2-032c-4377-92d5-9d7722f187d9",
   "metadata": {},
   "source": [
    "## Convert submited data to test_data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6e43f7f-979f-452c-a904-8603519d74cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 載入 CSV 檔案\n",
    "sample_submission_path = 'sample_submission_stage2.csv'\n",
    "station_info_path = 'station_info.csv'\n",
    "\n",
    "sample_submission = pd.read_csv(sample_submission_path)\n",
    "station_info = pd.read_csv(station_info_path)\n",
    "\n",
    "# 從 sample_submission 的 'id' 欄位中提取 'station_id'\n",
    "sample_submission['station_id'] = sample_submission['id'].str.split('_').str[1].astype(int)\n",
    "\n",
    "# 將 sample_submission 與 station_info 根據 'station_id' 合併\n",
    "merged_data = pd.merge(sample_submission, station_info, on='station_id', how='left')\n",
    "\n",
    "# 儲存合併後的資料為新的 CSV 檔案\n",
    "merged_data.to_csv('eva_marge.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "243623b6-6cc2-45e2-97f7-dfb98fea9e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2l/kqh7y27x3s7gdc5pbwkvy0xc0000gn/T/ipykernel_89334/2045974358.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reordered_eva_marge['sbi'] = None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 載入 CSV 檔案\n",
    "eva_marge_path = 'eva_marge.csv'\n",
    "eva_marge = pd.read_csv(eva_marge_path)\n",
    "\n",
    "# 從 'id' 欄位中提取 'date' 和 'time'\n",
    "eva_marge['date'] = eva_marge['id'].str.split('_').str[0].astype(int)\n",
    "eva_marge['time'] = eva_marge['id'].str.split('_').str[2].str.split(':').str[0].astype(int) * 60 + \\\n",
    "                    eva_marge['id'].str.split('_').str[2].str.split(':').str[1].astype(int)\n",
    "\n",
    "# 重新排列欄位以符合 samples.csv 的格式\n",
    "# 'sbi' 欄位設置為空並移至最右邊\n",
    "reordered_eva_marge = eva_marge[['date', 'station_id', 'time', 'latitude', 'longitude']]\n",
    "reordered_eva_marge['sbi'] = None\n",
    "\n",
    "# 儲存重組後的資料為新的 CSV 檔案\n",
    "reordered_eva_marge.to_csv('test_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "957c218f-8f9a-472d-8a69-c3b7b27195ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       date  station_id  time  latitude  longitude  sbi\n",
      "0  20231002   500101001     0  25.02605   121.5436   12\n",
      "1  20231002   500101001    20  25.02605   121.5436   12\n",
      "2  20231002   500101001    40  25.02605   121.5436   12\n",
      "3  20231002   500101001    60  25.02605   121.5436    8\n",
      "4  20231002   500101001    80  25.02605   121.5436    8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 讀取 CSV 文件的路徑\n",
    "# 請將以下路徑替換為您本地 CSV 文件的實際路徑\n",
    "train_data_path = 'train_data.csv'  # 例如 'C:/Users/YourName/Documents/train_data.csv'\n",
    "\n",
    "# 讀取 CSV 文件\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "\n",
    "# 顯示處理後的數據集\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceae9ba-f90d-472b-8cbc-a47656dd747b",
   "metadata": {},
   "source": [
    "## Main traing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84fedc7b-722b-4fca-9f9f-50e6b8f43a73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 61.0923\n",
      "Epoch [2/100], Loss: 58.5671\n",
      "Epoch [3/100], Loss: 57.3840\n",
      "Epoch [4/100], Loss: 56.6302\n",
      "Epoch [5/100], Loss: 56.0591\n",
      "Epoch [6/100], Loss: 55.5898\n",
      "Epoch [7/100], Loss: 55.2353\n",
      "Epoch [8/100], Loss: 54.9222\n",
      "Epoch [9/100], Loss: 54.6454\n",
      "Epoch [10/100], Loss: 54.3851\n",
      "Epoch [11/100], Loss: 54.1126\n",
      "Epoch [12/100], Loss: 53.8458\n",
      "Epoch [13/100], Loss: 53.6126\n",
      "Epoch [14/100], Loss: 53.3977\n",
      "Epoch [15/100], Loss: 53.2059\n",
      "Epoch [16/100], Loss: 53.0498\n",
      "Epoch [17/100], Loss: 52.9287\n",
      "Epoch [18/100], Loss: 52.8081\n",
      "Epoch [19/100], Loss: 52.6917\n",
      "Epoch [20/100], Loss: 52.5783\n",
      "Epoch [21/100], Loss: 52.4621\n",
      "Epoch [22/100], Loss: 52.3569\n",
      "Epoch [23/100], Loss: 52.2388\n",
      "Epoch [24/100], Loss: 52.1460\n",
      "Epoch [25/100], Loss: 52.0467\n",
      "Epoch [26/100], Loss: 51.9702\n",
      "Epoch [27/100], Loss: 51.8666\n",
      "Epoch [28/100], Loss: 51.7764\n",
      "Epoch [29/100], Loss: 51.6906\n",
      "Epoch [30/100], Loss: 51.6057\n",
      "Epoch [31/100], Loss: 51.5485\n",
      "Epoch [32/100], Loss: 51.4921\n",
      "Epoch [33/100], Loss: 51.4271\n",
      "Epoch [34/100], Loss: 51.3564\n",
      "Epoch [35/100], Loss: 51.2872\n",
      "Epoch [36/100], Loss: 51.2241\n",
      "Epoch [37/100], Loss: 51.1647\n",
      "Epoch [38/100], Loss: 51.1223\n",
      "Epoch [39/100], Loss: 51.0572\n",
      "Epoch [40/100], Loss: 51.0032\n",
      "Epoch [41/100], Loss: 50.9533\n",
      "Epoch [42/100], Loss: 50.8983\n",
      "Epoch [43/100], Loss: 50.8626\n",
      "Epoch [44/100], Loss: 50.8067\n",
      "Epoch [45/100], Loss: 50.7603\n",
      "Epoch [46/100], Loss: 50.7071\n",
      "Epoch [47/100], Loss: 50.6516\n",
      "Epoch [48/100], Loss: 50.6128\n",
      "Epoch [49/100], Loss: 50.5678\n",
      "Epoch [50/100], Loss: 50.5243\n",
      "Epoch [51/100], Loss: 50.4954\n",
      "Epoch [52/100], Loss: 50.4446\n",
      "Epoch [53/100], Loss: 50.4135\n",
      "Epoch [54/100], Loss: 50.3661\n",
      "Epoch [55/100], Loss: 50.3463\n",
      "Epoch [56/100], Loss: 50.3035\n",
      "Epoch [57/100], Loss: 50.2815\n",
      "Epoch [58/100], Loss: 50.2460\n",
      "Epoch [59/100], Loss: 50.2141\n",
      "Epoch [60/100], Loss: 50.1730\n",
      "Epoch [61/100], Loss: 50.1550\n",
      "Epoch [62/100], Loss: 50.1056\n",
      "Epoch [63/100], Loss: 50.0889\n",
      "Epoch [64/100], Loss: 50.0407\n",
      "Epoch [65/100], Loss: 50.0179\n",
      "Epoch [66/100], Loss: 49.9860\n",
      "Epoch [67/100], Loss: 49.9456\n",
      "Epoch [68/100], Loss: 49.9106\n",
      "Epoch [69/100], Loss: 49.8839\n",
      "Epoch [70/100], Loss: 49.8511\n",
      "Epoch [71/100], Loss: 49.7963\n",
      "Epoch [72/100], Loss: 49.7640\n",
      "Epoch [73/100], Loss: 49.7239\n",
      "Epoch [74/100], Loss: 49.6921\n",
      "Epoch [75/100], Loss: 49.6524\n",
      "Epoch [76/100], Loss: 49.6146\n",
      "Epoch [77/100], Loss: 49.5744\n",
      "Epoch [78/100], Loss: 49.5529\n",
      "Epoch [79/100], Loss: 49.5186\n",
      "Epoch [80/100], Loss: 49.4968\n",
      "Epoch [81/100], Loss: 49.4622\n",
      "Epoch [82/100], Loss: 49.4370\n",
      "Epoch [83/100], Loss: 49.4198\n",
      "Epoch [84/100], Loss: 49.3818\n",
      "Epoch [85/100], Loss: 49.3787\n",
      "Epoch [86/100], Loss: 49.3457\n",
      "Epoch [87/100], Loss: 49.3211\n",
      "Epoch [88/100], Loss: 49.3080\n",
      "Epoch [89/100], Loss: 49.2787\n",
      "Epoch [90/100], Loss: 49.2767\n",
      "Epoch [91/100], Loss: 49.2598\n",
      "Epoch [92/100], Loss: 49.2364\n",
      "Epoch [93/100], Loss: 49.2223\n",
      "Epoch [94/100], Loss: 49.2190\n",
      "Epoch [95/100], Loss: 49.1823\n",
      "Epoch [96/100], Loss: 49.1645\n",
      "Epoch [97/100], Loss: 49.1418\n",
      "Epoch [98/100], Loss: 49.1224\n",
      "Epoch [99/100], Loss: 49.1074\n",
      "Epoch [100/100], Loss: 49.0848\n",
      "均方誤差 (MSE): 48.70139333374706\n",
      "均方根誤差 (RMSE): 6.978638358143161\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# 設定裝置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 讀取 CSV 文件的路徑\n",
    "train_data_path = 'train_data.csv'  # 替換為您的文件路徑\n",
    "\n",
    "# 讀取數據\n",
    "data = pd.read_csv(train_data_path)\n",
    "\n",
    "# 選擇特徵和目標變量\n",
    "X = data[['date', 'station_id', 'time', 'latitude', 'longitude']].values\n",
    "y = data['sbi'].values\n",
    "\n",
    "# 拆分數據集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 數據標準化\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 轉換為 PyTorch 張量\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# 定義 PyTorch 數據集\n",
    "class TrainData(Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "train_data = TrainData(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "# 定義模型\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.layer_1 = nn.Linear(5, 64) \n",
    "        self.layer_2 = nn.Linear(64, 64)\n",
    "        self.layer_out = nn.Linear(64, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.layer_out(x)\n",
    "        return x\n",
    "\n",
    "model = NeuralNet().to(device)\n",
    "\n",
    "# 定義損失函數和優化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 訓練模型\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # 在每個 epoch 結束時打印平均損失\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# 在測試數據上評估模型\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_tensor = model(X_test_tensor)\n",
    "    y_pred = y_pred_tensor.detach().cpu().numpy()\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f\"均方誤差 (MSE): {mse}\")\n",
    "    print(f\"均方根誤差 (RMSE): {rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b386652-c2cc-4de8-a930-87eeb7fd4e36",
   "metadata": {},
   "source": [
    "## Predict test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "110a4ff6-fe45-433f-b4bb-a9c5cf6169cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 假設您已經有了一個訓練好的模型和對應的標準化對象 'scaler'\n",
    "\n",
    "# 加載新數據\n",
    "new_data_path = 'test_data.csv'  # 替換為新數據的文件路徑\n",
    "new_data = pd.read_csv(new_data_path)\n",
    "\n",
    "# 預處理新數據\n",
    "X_new = new_data[['date', 'station_id', 'time', 'latitude', 'longitude']].values\n",
    "X_new = scaler.transform(X_new)  # 使用之前訓練時使用的 scaler 對象\n",
    "X_new_tensor = torch.tensor(X_new, dtype=torch.float32).to(device)\n",
    "\n",
    "# 使用模型進行預測\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_new_pred_tensor = model(X_new_tensor)\n",
    "    y_new_pred = y_new_pred_tensor.detach().cpu().numpy()\n",
    "\n",
    "# 四捨五入並確保預測值非負\n",
    "y_new_pred_rounded = np.round(y_new_pred).astype(int)\n",
    "y_new_pred_rounded = np.clip(y_new_pred_rounded, 0, None)  # 確保所有預測值非負\n",
    "\n",
    "# 創建一個新的 DataFrame 來儲存預測結果\n",
    "predicted_data = pd.DataFrame(y_new_pred_rounded, columns=['predicted_sbi'])\n",
    "\n",
    "# 將預測結果保存到 CSV 文件\n",
    "predicted_data.to_csv('predicted_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532b0ff6-f4f6-4f23-9340-c09582f3731c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
