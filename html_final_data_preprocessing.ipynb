{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe804650-b386-4238-ad2a-eb1b1471f20a",
   "metadata": {},
   "source": [
    "# 預處理思路\n",
    "從驗證的public dataset中可以看到需要從日期、站點ID、時間資料去進行每20分鐘的可用腳踏車數量的預測，因此訓練資料集必須整理成含有以上資訊的資料集，因為保持數據集可處理的彈性，以下程式碼我會分階段進行，主要是把經緯度的資料加進去並去除無效資料，但我還是有讓處理過程都輸出成csv檔案，因為驗證的公式需要tot(停車柱的數量)的資料，最後的train_data有兩種主要是保留時序的彈性。\n",
    "\n",
    "訓練資料集處理後的特徵包含date, station_id, time, latitude, longitude希望用這五個資料去預測sbi，因此驗證數據集也需要整理成這格式，看之後模型要怎麼處理這些特徵值。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e4cd65-f4cf-403b-bc7c-7f8c4250608e",
   "metadata": {},
   "source": [
    "## ENV Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb802a77-b19a-4740-843f-cf88aea6f3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa974ed-2af0-4828-8397-89a4dc17a57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbd8bcb-6db0-4629-af2a-1b5f321511a7",
   "metadata": {},
   "source": [
    "## Data Processing and Feature Acquisition\n",
    "* 輸出檔名為aggregated_data_YYYYMMDD的csv檔案到根目錄，資料很多會花一些時間\n",
    "* 輸出完成後我手動在根目錄新建一個名為\"aggreated_data\"的資料夾把這些csv檔案都放進去 (我懶得用程式)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af0b90c-e650-4286-827c-45f8d39a24d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def convert_time_to_minutes(time_str):\n",
    "    hours, minutes = map(int, time_str.split(':'))\n",
    "    return hours * 60 + minutes\n",
    "\n",
    "def process_json_file(json_path, site_id):\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Get the sorted list of times from the data\n",
    "    sorted_times = sorted(data.keys(), key=lambda x: convert_time_to_minutes(x))\n",
    "\n",
    "    # Find the first non-empty data entry\n",
    "    for time in sorted_times:\n",
    "        if data[time]:\n",
    "            last_valid_data = data[time]\n",
    "            break\n",
    "    else:\n",
    "        last_valid_data = {'tot': 0, 'sbi': 0, 'bemp': 0, 'act': '0'}\n",
    "\n",
    "    # Process data for every 20 minutes interval\n",
    "    processed_data = []\n",
    "    for minutes in range(0, 24 * 60, 20):\n",
    "        current_time_str = f\"{minutes // 60:02d}:{minutes % 60:02d}\"\n",
    "        if current_time_str in data and data[current_time_str]:\n",
    "            last_valid_data = data[current_time_str]\n",
    "\n",
    "        processed_data.append({\n",
    "            'station_id': site_id,\n",
    "            'Time (minutes)': minutes,\n",
    "            'tot': last_valid_data.get('tot', 0),\n",
    "            'sbi': last_valid_data.get('sbi', 0),\n",
    "            'bemp': last_valid_data.get('bemp', 0),\n",
    "            'act': last_valid_data.get('act', '0')\n",
    "        })\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "def main():\n",
    "    base_path = '/data/html.2023.final.data-release/release/'\n",
    "    start_date = datetime(2023, 10, 2)\n",
    "    end_date = datetime(2023, 12, 8)\n",
    "\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        folder_name = current_date.strftime('%Y%m%d')\n",
    "        folder_path = os.path.join(base_path, folder_name)\n",
    "        if os.path.exists(folder_path):\n",
    "            all_data = []\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                if file_name.endswith('.json'):\n",
    "                    json_path = os.path.join(folder_path, file_name)\n",
    "                    # Extract site ID from file name\n",
    "                    site_id = file_name.split('.')[0]\n",
    "                    all_data.extend(process_json_file(json_path, site_id))\n",
    "\n",
    "            # Convert the aggregated data into a DataFrame\n",
    "            if all_data:\n",
    "                df = pd.DataFrame(all_data)\n",
    "                output_csv = f'/data/aggregated_data_{folder_name}.csv'\n",
    "                df.to_csv(output_csv, index=False)\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43624bf-4b8c-497e-9519-b9380bcf1b32",
   "metadata": {},
   "source": [
    "## Capture latitude and longitude data from demographic\n",
    "* 提取demographic檔案經緯度資料並輸出csv檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b3b168-4f35-443e-bb96-2337669d92c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The previous path used was incorrect. Let's correct the file path.\n",
    "demographic_file_path = '/data/html.2023.final.data-release/demographic.json'\n",
    "\n",
    "# Function to extract station ID, latitude, and longitude and output it as a CSV\n",
    "def extract_station_info_to_csv(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Prepare data for DataFrame\n",
    "    stations_data = [{\n",
    "        'station_id': sno,\n",
    "        'latitude': info['lat'],\n",
    "        'longitude': info['lng']\n",
    "    } for sno, info in data.items()]\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(stations_data)\n",
    "    \n",
    "    # Define the CSV output path\n",
    "    output_csv_path = '/data/station_info.csv'\n",
    "    \n",
    "    # Save as CSV\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    \n",
    "    return output_csv_path\n",
    "\n",
    "# Call the function and get the path of the created CSV\n",
    "csv_output_path = extract_station_info_to_csv(demographic_file_path)\n",
    "csv_output_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c64d3b-a730-4ca8-832c-00b714d802ff",
   "metadata": {},
   "source": [
    "## Batch aggreated data and merge date\n",
    "* 批次處理\"aggreated_data\"資料夾內的檔案並與經緯度合併 > 最後輸出一個包含所有資料的csv資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb45a9f-1a59-46b5-b68d-3e2f9b476f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the base directory containing the aggregated data files\n",
    "base_dir = '/data/aggreated_data/'\n",
    "\n",
    "# Load station information\n",
    "station_info_file_path = '/data/station_info.csv'\n",
    "station_info_df = pd.read_csv(station_info_file_path)\n",
    "station_info_df['station_id'] = station_info_df['station_id'].astype(int)\n",
    "\n",
    "# Initialize an empty DataFrame to store the merged data\n",
    "all_data_merged = pd.DataFrame()\n",
    "\n",
    "# Iterate through each file in the directory\n",
    "for file_name in os.listdir(base_dir):\n",
    "    if file_name.startswith('aggregated_data_') and file_name.endswith('.csv'):\n",
    "        # Extract the date from the filename\n",
    "        date_str = file_name[len('aggregated_data_'):-4]\n",
    "        file_path = os.path.join(base_dir, file_name)\n",
    "\n",
    "        # Load the aggregated data file\n",
    "        aggregated_data_df = pd.read_csv(file_path)\n",
    "        aggregated_data_df['station_id'] = aggregated_data_df['station_id'].astype(int)\n",
    "\n",
    "        # Add the extracted date to the DataFrame\n",
    "        # Convert date to the desired format without separators (YYYYMMDD)\n",
    "        aggregated_data_df['date'] = pd.to_datetime(date_str).strftime('%Y%m%d')\n",
    "\n",
    "        # Merge with station information\n",
    "        merged_df = pd.merge(aggregated_data_df, station_info_df, how='left', on='station_id')\n",
    "\n",
    "        # Append to the overall DataFrame\n",
    "        all_data_merged = pd.concat([all_data_merged, merged_df])\n",
    "\n",
    "# Reset the index of the final DataFrame\n",
    "all_data_merged.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save the final merged data to a new CSV file\n",
    "output_file_path = '/data/merged_all_data.csv'\n",
    "all_data_merged.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Output file path for download\n",
    "output_file_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05553d2b-c557-409b-9065-1d7fa6bcbcec",
   "metadata": {},
   "source": [
    "## Sort columns\n",
    "* 單純看不順眼sort一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f5c2d6-d862-40ad-9be2-2c5e9b761074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import the necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the final merged dataset again due to the environment reset\n",
    "final_merged_dataset_path = '/data/merged_all_data.csv'\n",
    "final_merged_dataset = pd.read_csv(final_merged_dataset_path)\n",
    "\n",
    "# Reorder the columns according to the new requirement\n",
    "final_merged_dataset = final_merged_dataset[['date', 'station_id', 'Time (minutes)', 'latitude', 'longitude', 'tot', 'sbi', 'bemp', 'act']]\n",
    "\n",
    "# Rename the columns for consistency\n",
    "final_merged_dataset.rename(columns={'Time (minutes)': 'time'}, inplace=True)\n",
    "\n",
    "# Sort the DataFrame by 'date' and 'station_id'\n",
    "final_merged_dataset.sort_values(by=['date', 'station_id'], inplace=True)\n",
    "\n",
    "# Save the sorted and reordered DataFrame to a new CSV file\n",
    "sorted_reordered_csv_path = '/data/merged_all_data_sort.csv'\n",
    "final_merged_dataset.to_csv(sorted_reordered_csv_path, index=False)\n",
    "\n",
    "sorted_reordered_csv_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6572d4c6-1877-46b9-8312-6f3fb419798d",
   "metadata": {},
   "source": [
    "## Remove tot, bemp, act\n",
    "* 去除三項值用以符合驗證資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d46e837-12a6-4a6a-8d96-53323e5fd834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = '/data/merged_all_data_sort.csv'  # Replace this with your file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Remove specified columns\n",
    "data_cleaned = data.drop(['tot', 'bemp', 'act'], axis=1)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "data_cleaned.to_csv('train_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca6fa46-523d-451d-bb97-0a1265e30407",
   "metadata": {},
   "source": [
    "## Convert minute into 00:00 (optional)\n",
    "* 將時間轉換成24小時制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd48501-0350-410c-b85a-594b63488141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = '/data/train_data.csv'  # Replace with your file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Function to convert time format\n",
    "def convert_time(time):\n",
    "    hours = time // 60\n",
    "    minutes = time % 60\n",
    "    return f\"{hours:02d}:{minutes:02d}\"\n",
    "\n",
    "# Apply the conversion to the 'time' column\n",
    "data['time'] = data['time'].apply(convert_time)\n",
    "\n",
    "# The 'time' column is now in the desired format\n",
    "# You can now work with this updated dataframe or save it to a new file\n",
    "# For example, to save:\n",
    "data.to_csv('train_data_time_convert.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e43f7f-979f-452c-a904-8603519d74cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
